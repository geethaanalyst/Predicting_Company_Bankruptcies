# -*- coding: utf-8 -*-
"""bankruptcies01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17MMAsymq-C7mIkGgo8GXv-hftol_r_w-

# Project Title: Predicting Company Bankruptcies
Description:
 Develop a robust machine learning model to accurately predict company bankruptcies. By utilizing this model, the institution aims to improve risk assessment, make informed lending decisions, and optimize their financial portfolio management.

# Import libraries
"""

#Import libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from imblearn.under_sampling import RandomUnderSampler
import warnings
warnings.filterwarnings('ignore')

"""# Load the data"""

df = pd.read_csv('Bankruptcies.csv')

"""# Data preprocessing"""

df.info()

df.describe()

print(df['Bankrupt?'].value_counts())

df.isnull().sum()

df.duplicated().sum()

"""# Univariate and Bivariate analysis"""

sns.countplot(data=df,x='Bankrupt?',palette='rainbow')

df.hist(figsize=(60,40))

#Correlation Matrix Heatmap
plt.figure(figsize=(10,6))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix of Features')
plt.show()

"""Visualizing the Effect of ROA on Bankruptcy using scatterplots"""

sns.scatterplot(data=df,x=df.index,y=' ROA(A) before interest and % after tax',hue='Bankrupt?',alpha=0.7,size=5)

sns.scatterplot(data=df,x=df.index,y=' ROA(B) before interest and depreciation after tax',hue='Bankrupt?',alpha=0.7,size=5)

sns.scatterplot(data=df,x=df.index,y=' Net Income to Total Assets',hue='Bankrupt?',alpha=0.7,size=5)

"""# Top Features correlates with bankruptcy"""

corr = df.corr()
top_features = corr['Bankrupt?'].abs().sort_values(ascending=False).iloc[1:11]
print(top_features)

#  Feature selection by correlation with target 'Bankrupt'
corr = df.corr()
top_features = corr['Bankrupt?'].abs().sort_values(ascending=False).iloc[1:11].index.tolist()
print("Top features correlated with bankruptcy:", top_features)

thres=0.2
fil_cols=[col for col in df.columns if abs(df.corr()['Bankrupt?'][col])>thres]
df_filt=df[fil_cols]
#Correlation Matrix Heatmap
sns.heatmap(df_filt.corr(),cmap='coolwarm',annot=True)

"""# Define feature and the target"""

# Data preprocessing - undersample majority class if imbalanced
X = df[top_features]
y = df['Bankrupt?']
rus = RandomUnderSampler(random_state=42)
X_res, y_res = rus.fit_resample(X, y)

print("Balanced target counts:")
print(pd.Series(y_res).value_counts())

# Normalize numerical features
scaler = StandardScaler()
scaler.fit(X_res)
X_res = scaler.transform(X_res)

"""# Train and test split"""

# Split data into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

"""# Logistic regression"""

# Logistic Regression
lr = LogisticRegression(max_iter=2000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_lr):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

# Confusion Matrix Visualization
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression Confusion Matrix')
plt.show()

"""# Decision tree classifier"""

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("\nDecision Tree Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_dt):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

# Confusion Matrix Visualization
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Blues')
plt.title('Decision Tree Confusion Matrix')
plt.show()

"""# Hyper parameter tuning and cross validation"""

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 2, 4, 6, 8, 10],
    'min_samples_split': [2, 5, 7],
    'min_samples_leaf': [1, 2, 4]
}

# Perform Grid Search with cross-validation (e.g., K=5)
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a Decision Tree classifier with the best hyperparameters
best_clf = DecisionTreeClassifier(random_state=42, **best_params)
best_clf.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = best_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Print the best hyperparameters and model accuracy
print(f"Best Hyperparameters: {best_params}")
print(f"Model Accuracy on Test Data: {accuracy:.2f}")

"""# Random forest classifier"""

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
print("Random Forest Classifier Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""#Ensemble Modeling with Voting Classifier"""

#  Ensemble Voting Classifier
ensemble = VotingClassifier(estimators=[
    ('lr', LogisticRegression(max_iter=2000)),
    ('rf', RandomForestClassifier(random_state=42)),
    ('svc', SVC(probability=True, random_state=42))
], voting='soft')
ensemble.fit(X_train, y_train)

#  Hyperparameter tuning example for Random Forest within ensemble
param_grid = {
    'rf__n_estimators': [100, 200],
    'rf__max_depth': [5, 10, None],
    'svc__C': [0.1, 1, 10]
}
search = RandomizedSearchCV(ensemble, param_grid, cv=3, scoring='f1', n_iter=5, random_state=42)
search.fit(X_train, y_train)
best_model = search.best_estimator_
print("Best ensemble parameters:", search.best_params_)

# Evaluate on test dataset
y_pred = best_model.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Test Precision:", precision_score(y_test, y_pred))
print("Test Recall:", recall_score(y_test, y_pred))
print("Test F1-score:", f1_score(y_test, y_pred))

"""# Hyper parameter tuning in random forest"""

# Parameter grid
param_grid = {
    'max_depth': [4,8,10,12],
    'min_samples_leaf': [2,4,6,8],
    'min_samples_split': [2,4,6,8],
    'criterion': ['gini', 'entropy']
}

"""Grid search and cross validation"""

# Perform Grid Search with cross-validation (e.g., K=5)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a Random forest classifier with the best hyperparameters
best_rfc = RandomForestClassifier(random_state=42, **best_params)
best_rfc.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = best_rfc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Print the best hyperparameters and model accuracy
print(f"Best Hyperparameters: {best_params}")
print(f"Model Accuracy on Test Data: {accuracy:.2f}")

"""# Support vector machine"""

# Support Vector machine
svm = SVC(probability=True, random_state=42)
svm.fit(X_train, y_train)

y_pred_svm = svm.predict(X_test)
print("\nSupport vector machine Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix Visualization
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Blues')
plt.title('Support vector machine Confusion Matrix')
plt.show()

"""# K-nearest neighbors(KNN)"""

from sklearn.neighbors import KNeighborsClassifier
# K-nearest neighbors(KNN)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_test)
print("\nK-Nearest Neighbors Classifier Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_knn))

# Confusion Matrix Visualization
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='d', cmap='Blues')
plt.title('K-nearest neighbors Confusion Matrix')
plt.show()